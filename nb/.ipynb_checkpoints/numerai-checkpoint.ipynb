{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, preprocessing, linear_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#simple dataset\n",
    "class SimpleDataSet(object):\n",
    "    def __init__(self, data, label, batch_size):\n",
    "        self._data = data\n",
    "        self._label = label\n",
    "        self._start = 0\n",
    "        self._batch_size = batch_size\n",
    "        assert (len(self._data) == len(self._label))\n",
    "        self._size = len(self._data)\n",
    "\n",
    "    def new_epoch(self):\n",
    "        self._start = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self._start\n",
    "        self._start = start + self._batch_size\n",
    "        if self._start >= self._size:\n",
    "            self._start = self._size - 1\n",
    "        return self._data[start:self._start], self._label[start:self._start]\n",
    "\n",
    "    def total_batch(self):\n",
    "        if len(self._data) % self._batch_size == 0:\n",
    "            return len(self._data) / self._batch_size\n",
    "        return len(self._data) / self._batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert numerai input to tfrecords\n",
    "# tfrecords_fn=\"numerai.train.tfrecords\"\n",
    "# Load the data from the CSV files\n",
    "training_data = pd.read_csv(\n",
    "    \\'../../nb/numerai1/numerai_training_data_.csv\\', header=0)\n",
    "prediction_data = pd.read_csv(\n",
    "    \\'../../nb/numerai1/numerai_tournament_data.csv\\', header=0)\n",
    "\n",
    "total_data = training_data.copy()\n",
    "training_data = total_data.sample(frac=0.8, random_state=1)\n",
    "test_data = total_data.loc[~total_data.index.isin(training_data.index)]\n",
    "\n",
    "# Transform the loaded CSV data into numpy arrays\n",
    "feas_data = training_data.drop(\\'target\\', axis=1)\n",
    "label_data = training_data[\\'target\\']\n",
    "test_feas_data = test_data.drop(\\'target\\', axis=1)\n",
    "test_label_data = test_data[\\'target\\']\n",
    "tid_data = prediction_data[\\'t_id\\']\n",
    "tour_data = prediction_data.drop(\\'t_id\\', axis=1)\n",
    "_train_feas = feas_data.values\n",
    "_train_label = label_data.values\n",
    "_test_feas = test_feas_data.values\n",
    "_test_label = test_label_data.values\n",
    "_tour_feas = tour_data.values\n",
    "_tid = tid_data.values\n",
    "\n",
    "onehot_option = 1\n",
    "normal_option = 0\n",
    "\n",
    "_train_onehot_label = []\n",
    "_test_onehot_label = []\n",
    "onehot_op = tf.one_hot(\n",
    "    indices=_train_label, on_value=1., off_value=0., depth=2)\n",
    "test_onehot_op = tf.one_hot(\n",
    "    indices=_test_label, on_value=1., off_value=0., depth=2)\n",
    "with tf.Session() as sess:\n",
    "    if onehot_option is 1:\n",
    "        _train_onehot_label = onehot_op.eval()  #\n",
    "        _test_onehot_label = test_onehot_op.eval()\n",
    "    else:\n",
    "        _train_onehot_label = np.reshape(_train_label, (-1, 1))\n",
    "        _test_onehot_label = np.reshape(_test_label, (-1, 1))\n",
    "\n",
    "\n",
    "def feature_normalize(features):\n",
    "    mu = np.mean(features, axis=0)\n",
    "    sigma = np.std(features, axis=0)\n",
    "    return (features - mu) / sigma\n",
    "\n",
    "\n",
    "if normal_option is 1:\n",
    "    _train_feas = feature_normalize(_train_feas)\n",
    "    _tour_feas = feature_normalize(_tour_feas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27315, 51)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature42</th>\n",
       "      <th>feature43</th>\n",
       "      <th>feature44</th>\n",
       "      <th>feature45</th>\n",
       "      <th>feature46</th>\n",
       "      <th>feature47</th>\n",
       "      <th>feature48</th>\n",
       "      <th>feature49</th>\n",
       "      <th>feature50</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.547752</td>\n",
       "      <td>0.188399</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.271940</td>\n",
       "      <td>-0.115925</td>\n",
       "      <td>0.464401</td>\n",
       "      <td>0.038626</td>\n",
       "      <td>-0.069747</td>\n",
       "      <td>0.401728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.301285</td>\n",
       "      <td>-0.403912</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.190477</td>\n",
       "      <td>-0.104100</td>\n",
       "      <td>0.060931</td>\n",
       "      <td>0.399299</td>\n",
       "      <td>-0.188582</td>\n",
       "      <td>0.202357</td>\n",
       "      <td>0.010096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature2</th>\n",
       "      <td>-0.547752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.385038</td>\n",
       "      <td>-0.027149</td>\n",
       "      <td>-0.250862</td>\n",
       "      <td>0.293933</td>\n",
       "      <td>-0.139284</td>\n",
       "      <td>0.491165</td>\n",
       "      <td>-0.031156</td>\n",
       "      <td>-0.229350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470873</td>\n",
       "      <td>-0.028080</td>\n",
       "      <td>-0.268174</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.344950</td>\n",
       "      <td>0.125439</td>\n",
       "      <td>-0.223892</td>\n",
       "      <td>-0.243075</td>\n",
       "      <td>-0.268161</td>\n",
       "      <td>-0.012585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature3</th>\n",
       "      <td>0.188399</td>\n",
       "      <td>-0.385038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141428</td>\n",
       "      <td>0.547272</td>\n",
       "      <td>-0.574971</td>\n",
       "      <td>0.232475</td>\n",
       "      <td>-0.057986</td>\n",
       "      <td>-0.607304</td>\n",
       "      <td>0.312914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373919</td>\n",
       "      <td>0.132449</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>0.292331</td>\n",
       "      <td>0.149504</td>\n",
       "      <td>0.348877</td>\n",
       "      <td>0.503185</td>\n",
       "      <td>-0.056034</td>\n",
       "      <td>0.129323</td>\n",
       "      <td>0.035380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature4</th>\n",
       "      <td>0.012195</td>\n",
       "      <td>-0.027149</td>\n",
       "      <td>0.141428</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.472861</td>\n",
       "      <td>-0.132787</td>\n",
       "      <td>-0.081232</td>\n",
       "      <td>-0.596228</td>\n",
       "      <td>-0.090177</td>\n",
       "      <td>-0.388336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046970</td>\n",
       "      <td>0.718092</td>\n",
       "      <td>0.126124</td>\n",
       "      <td>0.500078</td>\n",
       "      <td>0.282742</td>\n",
       "      <td>0.046970</td>\n",
       "      <td>-0.042374</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>0.635071</td>\n",
       "      <td>0.009333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature5</th>\n",
       "      <td>0.271940</td>\n",
       "      <td>-0.250862</td>\n",
       "      <td>0.547272</td>\n",
       "      <td>0.472861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.433613</td>\n",
       "      <td>-0.296846</td>\n",
       "      <td>-0.386862</td>\n",
       "      <td>0.454504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120184</td>\n",
       "      <td>0.364581</td>\n",
       "      <td>-0.158210</td>\n",
       "      <td>0.795651</td>\n",
       "      <td>0.363971</td>\n",
       "      <td>0.672749</td>\n",
       "      <td>0.371995</td>\n",
       "      <td>0.220235</td>\n",
       "      <td>0.205222</td>\n",
       "      <td>0.036046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature6</th>\n",
       "      <td>-0.115925</td>\n",
       "      <td>0.293933</td>\n",
       "      <td>-0.574971</td>\n",
       "      <td>-0.132787</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.096930</td>\n",
       "      <td>0.177762</td>\n",
       "      <td>-0.008506</td>\n",
       "      <td>0.049290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311639</td>\n",
       "      <td>-0.015387</td>\n",
       "      <td>-0.424147</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>-0.210808</td>\n",
       "      <td>0.274445</td>\n",
       "      <td>-0.268461</td>\n",
       "      <td>-0.141161</td>\n",
       "      <td>-0.157253</td>\n",
       "      <td>-0.017276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature7</th>\n",
       "      <td>0.464401</td>\n",
       "      <td>-0.139284</td>\n",
       "      <td>0.232475</td>\n",
       "      <td>-0.081232</td>\n",
       "      <td>0.433613</td>\n",
       "      <td>-0.096930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.106962</td>\n",
       "      <td>-0.086672</td>\n",
       "      <td>0.664223</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207286</td>\n",
       "      <td>-0.323663</td>\n",
       "      <td>0.105679</td>\n",
       "      <td>0.449574</td>\n",
       "      <td>0.105143</td>\n",
       "      <td>0.440815</td>\n",
       "      <td>0.275172</td>\n",
       "      <td>-0.205968</td>\n",
       "      <td>-0.160077</td>\n",
       "      <td>0.032228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature8</th>\n",
       "      <td>0.038626</td>\n",
       "      <td>0.491165</td>\n",
       "      <td>-0.057986</td>\n",
       "      <td>-0.596228</td>\n",
       "      <td>-0.296846</td>\n",
       "      <td>0.177762</td>\n",
       "      <td>0.106962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.184180</td>\n",
       "      <td>0.212485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245033</td>\n",
       "      <td>-0.611339</td>\n",
       "      <td>-0.084469</td>\n",
       "      <td>-0.150242</td>\n",
       "      <td>-0.609417</td>\n",
       "      <td>0.095215</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>-0.724489</td>\n",
       "      <td>-0.390877</td>\n",
       "      <td>-0.006604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature9</th>\n",
       "      <td>-0.069747</td>\n",
       "      <td>-0.031156</td>\n",
       "      <td>-0.607304</td>\n",
       "      <td>-0.090177</td>\n",
       "      <td>-0.386862</td>\n",
       "      <td>-0.008506</td>\n",
       "      <td>-0.086672</td>\n",
       "      <td>-0.184180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.345872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383006</td>\n",
       "      <td>-0.123735</td>\n",
       "      <td>0.214562</td>\n",
       "      <td>-0.157244</td>\n",
       "      <td>0.382881</td>\n",
       "      <td>-0.650355</td>\n",
       "      <td>-0.381149</td>\n",
       "      <td>0.336423</td>\n",
       "      <td>0.029528</td>\n",
       "      <td>-0.015055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature10</th>\n",
       "      <td>0.401728</td>\n",
       "      <td>-0.229350</td>\n",
       "      <td>0.312914</td>\n",
       "      <td>-0.388336</td>\n",
       "      <td>0.454504</td>\n",
       "      <td>0.049290</td>\n",
       "      <td>0.664223</td>\n",
       "      <td>0.212485</td>\n",
       "      <td>-0.345872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.357903</td>\n",
       "      <td>-0.327708</td>\n",
       "      <td>-0.076684</td>\n",
       "      <td>0.248379</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>0.703493</td>\n",
       "      <td>0.571077</td>\n",
       "      <td>-0.375442</td>\n",
       "      <td>-0.494857</td>\n",
       "      <td>0.023206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature11</th>\n",
       "      <td>0.096138</td>\n",
       "      <td>0.019389</td>\n",
       "      <td>-0.135064</td>\n",
       "      <td>-0.530351</td>\n",
       "      <td>-0.643403</td>\n",
       "      <td>0.078531</td>\n",
       "      <td>-0.258756</td>\n",
       "      <td>0.460327</td>\n",
       "      <td>-0.199376</td>\n",
       "      <td>-0.014488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049672</td>\n",
       "      <td>-0.493935</td>\n",
       "      <td>-0.078923</td>\n",
       "      <td>-0.706883</td>\n",
       "      <td>-0.672002</td>\n",
       "      <td>-0.154228</td>\n",
       "      <td>0.079113</td>\n",
       "      <td>-0.514311</td>\n",
       "      <td>-0.185313</td>\n",
       "      <td>-0.036691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature12</th>\n",
       "      <td>0.609829</td>\n",
       "      <td>-0.515656</td>\n",
       "      <td>0.408279</td>\n",
       "      <td>0.051377</td>\n",
       "      <td>0.306373</td>\n",
       "      <td>-0.463833</td>\n",
       "      <td>0.376367</td>\n",
       "      <td>-0.082645</td>\n",
       "      <td>-0.077638</td>\n",
       "      <td>0.349409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593486</td>\n",
       "      <td>-0.013211</td>\n",
       "      <td>0.372603</td>\n",
       "      <td>0.203580</td>\n",
       "      <td>0.302833</td>\n",
       "      <td>0.092590</td>\n",
       "      <td>0.257077</td>\n",
       "      <td>-0.194640</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>0.027212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature13</th>\n",
       "      <td>0.338302</td>\n",
       "      <td>0.244579</td>\n",
       "      <td>0.099412</td>\n",
       "      <td>-0.103372</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>-0.259559</td>\n",
       "      <td>0.326220</td>\n",
       "      <td>0.355070</td>\n",
       "      <td>-0.178748</td>\n",
       "      <td>0.323559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052961</td>\n",
       "      <td>-0.497880</td>\n",
       "      <td>0.370726</td>\n",
       "      <td>0.137964</td>\n",
       "      <td>-0.171727</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.548222</td>\n",
       "      <td>-0.368883</td>\n",
       "      <td>0.028862</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature14</th>\n",
       "      <td>0.120244</td>\n",
       "      <td>0.090354</td>\n",
       "      <td>0.036521</td>\n",
       "      <td>-0.633142</td>\n",
       "      <td>-0.104831</td>\n",
       "      <td>-0.179031</td>\n",
       "      <td>0.487488</td>\n",
       "      <td>0.372366</td>\n",
       "      <td>0.101307</td>\n",
       "      <td>0.543448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.551525</td>\n",
       "      <td>-0.006458</td>\n",
       "      <td>-0.054832</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.199166</td>\n",
       "      <td>0.293512</td>\n",
       "      <td>-0.606318</td>\n",
       "      <td>-0.560935</td>\n",
       "      <td>0.008779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature15</th>\n",
       "      <td>0.219771</td>\n",
       "      <td>-0.060663</td>\n",
       "      <td>0.367842</td>\n",
       "      <td>0.534545</td>\n",
       "      <td>0.464247</td>\n",
       "      <td>-0.163877</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>-0.290137</td>\n",
       "      <td>-0.583721</td>\n",
       "      <td>0.121013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.559008</td>\n",
       "      <td>0.509848</td>\n",
       "      <td>0.044824</td>\n",
       "      <td>0.265767</td>\n",
       "      <td>0.017364</td>\n",
       "      <td>0.440070</td>\n",
       "      <td>0.249114</td>\n",
       "      <td>0.037364</td>\n",
       "      <td>0.086560</td>\n",
       "      <td>0.016418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature16</th>\n",
       "      <td>-0.443525</td>\n",
       "      <td>-0.086969</td>\n",
       "      <td>-0.070531</td>\n",
       "      <td>-0.270930</td>\n",
       "      <td>-0.342212</td>\n",
       "      <td>0.200168</td>\n",
       "      <td>-0.330463</td>\n",
       "      <td>-0.190381</td>\n",
       "      <td>0.012713</td>\n",
       "      <td>-0.072481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178910</td>\n",
       "      <td>0.175948</td>\n",
       "      <td>-0.327939</td>\n",
       "      <td>-0.448799</td>\n",
       "      <td>0.082604</td>\n",
       "      <td>-0.135502</td>\n",
       "      <td>-0.176042</td>\n",
       "      <td>0.049640</td>\n",
       "      <td>-0.431017</td>\n",
       "      <td>-0.009206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature17</th>\n",
       "      <td>0.345141</td>\n",
       "      <td>-0.222458</td>\n",
       "      <td>0.220499</td>\n",
       "      <td>0.666539</td>\n",
       "      <td>0.418473</td>\n",
       "      <td>-0.171863</td>\n",
       "      <td>-0.119200</td>\n",
       "      <td>-0.409720</td>\n",
       "      <td>-0.285525</td>\n",
       "      <td>0.013674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248691</td>\n",
       "      <td>0.373536</td>\n",
       "      <td>0.304674</td>\n",
       "      <td>0.283194</td>\n",
       "      <td>0.042967</td>\n",
       "      <td>0.203251</td>\n",
       "      <td>0.413869</td>\n",
       "      <td>0.339297</td>\n",
       "      <td>0.518184</td>\n",
       "      <td>-0.002994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature18</th>\n",
       "      <td>0.597160</td>\n",
       "      <td>-0.088139</td>\n",
       "      <td>0.296481</td>\n",
       "      <td>0.392465</td>\n",
       "      <td>0.557378</td>\n",
       "      <td>-0.177745</td>\n",
       "      <td>0.373224</td>\n",
       "      <td>0.019878</td>\n",
       "      <td>-0.257231</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015496</td>\n",
       "      <td>-0.166149</td>\n",
       "      <td>0.410553</td>\n",
       "      <td>0.565765</td>\n",
       "      <td>-0.025856</td>\n",
       "      <td>0.299506</td>\n",
       "      <td>0.638493</td>\n",
       "      <td>0.065109</td>\n",
       "      <td>0.406822</td>\n",
       "      <td>0.013050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature19</th>\n",
       "      <td>-0.435983</td>\n",
       "      <td>0.151814</td>\n",
       "      <td>-0.169533</td>\n",
       "      <td>0.263045</td>\n",
       "      <td>-0.208089</td>\n",
       "      <td>0.066550</td>\n",
       "      <td>-0.506505</td>\n",
       "      <td>-0.106938</td>\n",
       "      <td>0.422063</td>\n",
       "      <td>-0.702992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534566</td>\n",
       "      <td>0.275250</td>\n",
       "      <td>-0.177407</td>\n",
       "      <td>-0.006232</td>\n",
       "      <td>0.226769</td>\n",
       "      <td>-0.453859</td>\n",
       "      <td>-0.386351</td>\n",
       "      <td>0.442040</td>\n",
       "      <td>0.354869</td>\n",
       "      <td>-0.013408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature20</th>\n",
       "      <td>0.321865</td>\n",
       "      <td>0.298031</td>\n",
       "      <td>-0.003482</td>\n",
       "      <td>-0.283698</td>\n",
       "      <td>0.083797</td>\n",
       "      <td>-0.185825</td>\n",
       "      <td>0.389223</td>\n",
       "      <td>0.498644</td>\n",
       "      <td>-0.080854</td>\n",
       "      <td>0.394818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176314</td>\n",
       "      <td>-0.562458</td>\n",
       "      <td>0.192993</td>\n",
       "      <td>0.167995</td>\n",
       "      <td>-0.287653</td>\n",
       "      <td>0.180481</td>\n",
       "      <td>0.381888</td>\n",
       "      <td>-0.477385</td>\n",
       "      <td>-0.197078</td>\n",
       "      <td>0.004128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature21</th>\n",
       "      <td>-0.463633</td>\n",
       "      <td>-0.044965</td>\n",
       "      <td>-0.047522</td>\n",
       "      <td>0.566418</td>\n",
       "      <td>-0.131836</td>\n",
       "      <td>-0.121386</td>\n",
       "      <td>-0.643704</td>\n",
       "      <td>-0.524144</td>\n",
       "      <td>0.154246</td>\n",
       "      <td>-0.804474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148991</td>\n",
       "      <td>0.612009</td>\n",
       "      <td>-0.041487</td>\n",
       "      <td>-0.089474</td>\n",
       "      <td>0.234535</td>\n",
       "      <td>-0.442029</td>\n",
       "      <td>-0.407778</td>\n",
       "      <td>0.602993</td>\n",
       "      <td>0.568186</td>\n",
       "      <td>-0.016647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature22</th>\n",
       "      <td>-0.376396</td>\n",
       "      <td>0.321991</td>\n",
       "      <td>-0.381628</td>\n",
       "      <td>-0.141018</td>\n",
       "      <td>-0.220408</td>\n",
       "      <td>0.188880</td>\n",
       "      <td>-0.050567</td>\n",
       "      <td>0.130455</td>\n",
       "      <td>0.306860</td>\n",
       "      <td>-0.015280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072852</td>\n",
       "      <td>0.236472</td>\n",
       "      <td>-0.073863</td>\n",
       "      <td>-0.050258</td>\n",
       "      <td>0.087745</td>\n",
       "      <td>-0.078268</td>\n",
       "      <td>-0.504756</td>\n",
       "      <td>0.192214</td>\n",
       "      <td>-0.525926</td>\n",
       "      <td>0.010204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature23</th>\n",
       "      <td>-0.335533</td>\n",
       "      <td>-0.139439</td>\n",
       "      <td>-0.084246</td>\n",
       "      <td>-0.271761</td>\n",
       "      <td>-0.103928</td>\n",
       "      <td>0.313055</td>\n",
       "      <td>-0.054837</td>\n",
       "      <td>-0.251907</td>\n",
       "      <td>-0.102044</td>\n",
       "      <td>0.094974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029624</td>\n",
       "      <td>0.063716</td>\n",
       "      <td>-0.648455</td>\n",
       "      <td>-0.296552</td>\n",
       "      <td>0.017636</td>\n",
       "      <td>0.231548</td>\n",
       "      <td>-0.068975</td>\n",
       "      <td>-0.084954</td>\n",
       "      <td>-0.259587</td>\n",
       "      <td>-0.016470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature24</th>\n",
       "      <td>0.371690</td>\n",
       "      <td>0.067753</td>\n",
       "      <td>0.035046</td>\n",
       "      <td>-0.502425</td>\n",
       "      <td>0.020155</td>\n",
       "      <td>0.076618</td>\n",
       "      <td>0.391690</td>\n",
       "      <td>0.596109</td>\n",
       "      <td>-0.149037</td>\n",
       "      <td>0.583846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149569</td>\n",
       "      <td>-0.685877</td>\n",
       "      <td>-0.068459</td>\n",
       "      <td>0.053118</td>\n",
       "      <td>-0.353047</td>\n",
       "      <td>0.228330</td>\n",
       "      <td>0.512966</td>\n",
       "      <td>-0.450583</td>\n",
       "      <td>-0.365182</td>\n",
       "      <td>-0.001083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature25</th>\n",
       "      <td>0.370928</td>\n",
       "      <td>-0.051172</td>\n",
       "      <td>-0.099386</td>\n",
       "      <td>-0.548653</td>\n",
       "      <td>-0.202895</td>\n",
       "      <td>-0.107014</td>\n",
       "      <td>0.186299</td>\n",
       "      <td>0.291253</td>\n",
       "      <td>-0.017990</td>\n",
       "      <td>0.534630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330374</td>\n",
       "      <td>-0.448970</td>\n",
       "      <td>0.182005</td>\n",
       "      <td>-0.329146</td>\n",
       "      <td>-0.231018</td>\n",
       "      <td>0.119358</td>\n",
       "      <td>0.273669</td>\n",
       "      <td>-0.594811</td>\n",
       "      <td>-0.551928</td>\n",
       "      <td>-0.005370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature26</th>\n",
       "      <td>-0.457756</td>\n",
       "      <td>0.570357</td>\n",
       "      <td>-0.395298</td>\n",
       "      <td>0.259975</td>\n",
       "      <td>-0.304164</td>\n",
       "      <td>0.115513</td>\n",
       "      <td>-0.326635</td>\n",
       "      <td>0.157821</td>\n",
       "      <td>0.112881</td>\n",
       "      <td>-0.506269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178952</td>\n",
       "      <td>0.379910</td>\n",
       "      <td>-0.161853</td>\n",
       "      <td>-0.142573</td>\n",
       "      <td>-0.240645</td>\n",
       "      <td>-0.112629</td>\n",
       "      <td>-0.588743</td>\n",
       "      <td>0.177397</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>-0.012595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature27</th>\n",
       "      <td>-0.176165</td>\n",
       "      <td>0.030595</td>\n",
       "      <td>0.547509</td>\n",
       "      <td>-0.331018</td>\n",
       "      <td>0.400177</td>\n",
       "      <td>-0.068812</td>\n",
       "      <td>0.371792</td>\n",
       "      <td>0.254185</td>\n",
       "      <td>-0.347971</td>\n",
       "      <td>0.521857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137821</td>\n",
       "      <td>0.013982</td>\n",
       "      <td>-0.456938</td>\n",
       "      <td>0.277968</td>\n",
       "      <td>0.218336</td>\n",
       "      <td>0.535161</td>\n",
       "      <td>0.133158</td>\n",
       "      <td>-0.421144</td>\n",
       "      <td>-0.523829</td>\n",
       "      <td>0.033405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature28</th>\n",
       "      <td>0.092414</td>\n",
       "      <td>-0.000931</td>\n",
       "      <td>0.043680</td>\n",
       "      <td>-0.745982</td>\n",
       "      <td>-0.398886</td>\n",
       "      <td>-0.060868</td>\n",
       "      <td>0.196650</td>\n",
       "      <td>0.478430</td>\n",
       "      <td>-0.131505</td>\n",
       "      <td>0.402385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394089</td>\n",
       "      <td>-0.425787</td>\n",
       "      <td>-0.017666</td>\n",
       "      <td>-0.513683</td>\n",
       "      <td>-0.357921</td>\n",
       "      <td>0.034160</td>\n",
       "      <td>-0.058061</td>\n",
       "      <td>-0.600755</td>\n",
       "      <td>-0.743030</td>\n",
       "      <td>0.005898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature29</th>\n",
       "      <td>0.167702</td>\n",
       "      <td>0.086193</td>\n",
       "      <td>-0.384840</td>\n",
       "      <td>-0.194567</td>\n",
       "      <td>-0.659489</td>\n",
       "      <td>0.052627</td>\n",
       "      <td>-0.366428</td>\n",
       "      <td>0.381647</td>\n",
       "      <td>0.191786</td>\n",
       "      <td>-0.315199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068138</td>\n",
       "      <td>-0.301915</td>\n",
       "      <td>0.409904</td>\n",
       "      <td>-0.570801</td>\n",
       "      <td>-0.609706</td>\n",
       "      <td>-0.512510</td>\n",
       "      <td>-0.259856</td>\n",
       "      <td>-0.167601</td>\n",
       "      <td>0.069712</td>\n",
       "      <td>-0.031137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature30</th>\n",
       "      <td>0.178554</td>\n",
       "      <td>-0.134153</td>\n",
       "      <td>-0.296396</td>\n",
       "      <td>-0.009317</td>\n",
       "      <td>-0.418431</td>\n",
       "      <td>-0.102350</td>\n",
       "      <td>-0.225260</td>\n",
       "      <td>0.108075</td>\n",
       "      <td>0.606516</td>\n",
       "      <td>-0.449004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308751</td>\n",
       "      <td>-0.294692</td>\n",
       "      <td>0.507735</td>\n",
       "      <td>-0.197405</td>\n",
       "      <td>-0.021048</td>\n",
       "      <td>-0.718971</td>\n",
       "      <td>-0.227203</td>\n",
       "      <td>0.325071</td>\n",
       "      <td>0.417717</td>\n",
       "      <td>-0.016066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature31</th>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.013034</td>\n",
       "      <td>0.044561</td>\n",
       "      <td>-0.035395</td>\n",
       "      <td>0.148133</td>\n",
       "      <td>0.320420</td>\n",
       "      <td>0.034278</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>-0.285493</td>\n",
       "      <td>0.227801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242642</td>\n",
       "      <td>-0.247544</td>\n",
       "      <td>-0.225504</td>\n",
       "      <td>0.193893</td>\n",
       "      <td>-0.093883</td>\n",
       "      <td>0.286255</td>\n",
       "      <td>0.537011</td>\n",
       "      <td>-0.119001</td>\n",
       "      <td>0.138403</td>\n",
       "      <td>-0.017199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature32</th>\n",
       "      <td>-0.754212</td>\n",
       "      <td>0.296151</td>\n",
       "      <td>-0.327813</td>\n",
       "      <td>0.088829</td>\n",
       "      <td>-0.306245</td>\n",
       "      <td>-0.048163</td>\n",
       "      <td>-0.391524</td>\n",
       "      <td>-0.287372</td>\n",
       "      <td>0.554866</td>\n",
       "      <td>-0.574009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322856</td>\n",
       "      <td>0.354040</td>\n",
       "      <td>-0.146960</td>\n",
       "      <td>-0.135110</td>\n",
       "      <td>0.468982</td>\n",
       "      <td>-0.405273</td>\n",
       "      <td>-0.503091</td>\n",
       "      <td>0.375886</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>-0.006344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature33</th>\n",
       "      <td>0.455294</td>\n",
       "      <td>-0.208393</td>\n",
       "      <td>-0.199649</td>\n",
       "      <td>-0.214642</td>\n",
       "      <td>-0.265098</td>\n",
       "      <td>0.043849</td>\n",
       "      <td>0.355053</td>\n",
       "      <td>0.237271</td>\n",
       "      <td>0.153278</td>\n",
       "      <td>0.240058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352497</td>\n",
       "      <td>-0.429490</td>\n",
       "      <td>0.552942</td>\n",
       "      <td>-0.287626</td>\n",
       "      <td>-0.317201</td>\n",
       "      <td>-0.157589</td>\n",
       "      <td>-0.052200</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>-0.023694</td>\n",
       "      <td>0.000918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature34</th>\n",
       "      <td>-0.553739</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>-0.366426</td>\n",
       "      <td>-0.064459</td>\n",
       "      <td>-0.440402</td>\n",
       "      <td>0.136790</td>\n",
       "      <td>-0.351830</td>\n",
       "      <td>-0.312179</td>\n",
       "      <td>0.481213</td>\n",
       "      <td>-0.427410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099361</td>\n",
       "      <td>0.319832</td>\n",
       "      <td>-0.229803</td>\n",
       "      <td>-0.406258</td>\n",
       "      <td>0.235057</td>\n",
       "      <td>-0.348549</td>\n",
       "      <td>-0.584819</td>\n",
       "      <td>0.338731</td>\n",
       "      <td>-0.110890</td>\n",
       "      <td>-0.012833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature35</th>\n",
       "      <td>-0.420097</td>\n",
       "      <td>0.350151</td>\n",
       "      <td>-0.588666</td>\n",
       "      <td>-0.100342</td>\n",
       "      <td>-0.415898</td>\n",
       "      <td>0.464779</td>\n",
       "      <td>-0.402600</td>\n",
       "      <td>0.072646</td>\n",
       "      <td>0.065415</td>\n",
       "      <td>-0.230045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008314</td>\n",
       "      <td>0.300298</td>\n",
       "      <td>-0.330672</td>\n",
       "      <td>-0.452129</td>\n",
       "      <td>-0.340098</td>\n",
       "      <td>-0.030541</td>\n",
       "      <td>-0.632799</td>\n",
       "      <td>-0.051414</td>\n",
       "      <td>-0.364611</td>\n",
       "      <td>-0.023753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature36</th>\n",
       "      <td>0.321800</td>\n",
       "      <td>-0.272734</td>\n",
       "      <td>0.193165</td>\n",
       "      <td>-0.677828</td>\n",
       "      <td>0.053173</td>\n",
       "      <td>0.053209</td>\n",
       "      <td>0.360115</td>\n",
       "      <td>0.510750</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>0.583749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221801</td>\n",
       "      <td>-0.509241</td>\n",
       "      <td>0.058517</td>\n",
       "      <td>0.054418</td>\n",
       "      <td>0.053026</td>\n",
       "      <td>0.137909</td>\n",
       "      <td>0.129580</td>\n",
       "      <td>-0.410949</td>\n",
       "      <td>-0.404024</td>\n",
       "      <td>0.021873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature37</th>\n",
       "      <td>-0.269806</td>\n",
       "      <td>0.194719</td>\n",
       "      <td>-0.494252</td>\n",
       "      <td>-0.640702</td>\n",
       "      <td>-0.428423</td>\n",
       "      <td>0.369407</td>\n",
       "      <td>-0.033279</td>\n",
       "      <td>0.326909</td>\n",
       "      <td>0.451322</td>\n",
       "      <td>0.131779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125249</td>\n",
       "      <td>-0.189610</td>\n",
       "      <td>-0.319741</td>\n",
       "      <td>-0.250507</td>\n",
       "      <td>0.070341</td>\n",
       "      <td>-0.142086</td>\n",
       "      <td>-0.371319</td>\n",
       "      <td>-0.267897</td>\n",
       "      <td>-0.766108</td>\n",
       "      <td>-0.007159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature38</th>\n",
       "      <td>0.372019</td>\n",
       "      <td>-0.277500</td>\n",
       "      <td>0.764721</td>\n",
       "      <td>0.044792</td>\n",
       "      <td>0.503866</td>\n",
       "      <td>-0.480331</td>\n",
       "      <td>0.405592</td>\n",
       "      <td>0.044537</td>\n",
       "      <td>-0.553116</td>\n",
       "      <td>0.382169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.360839</td>\n",
       "      <td>-0.154822</td>\n",
       "      <td>0.319292</td>\n",
       "      <td>0.278128</td>\n",
       "      <td>-0.013280</td>\n",
       "      <td>0.358386</td>\n",
       "      <td>0.456054</td>\n",
       "      <td>-0.151016</td>\n",
       "      <td>0.269645</td>\n",
       "      <td>0.034330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature39</th>\n",
       "      <td>0.127682</td>\n",
       "      <td>-0.080586</td>\n",
       "      <td>0.196497</td>\n",
       "      <td>-0.238776</td>\n",
       "      <td>-0.068277</td>\n",
       "      <td>-0.610826</td>\n",
       "      <td>0.250375</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>0.256453</td>\n",
       "      <td>0.174346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004222</td>\n",
       "      <td>-0.373372</td>\n",
       "      <td>0.458122</td>\n",
       "      <td>-0.088522</td>\n",
       "      <td>0.132599</td>\n",
       "      <td>-0.146037</td>\n",
       "      <td>0.271292</td>\n",
       "      <td>-0.078685</td>\n",
       "      <td>0.029344</td>\n",
       "      <td>0.008912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature40</th>\n",
       "      <td>-0.043111</td>\n",
       "      <td>0.154129</td>\n",
       "      <td>-0.187030</td>\n",
       "      <td>-0.371355</td>\n",
       "      <td>-0.470603</td>\n",
       "      <td>0.051060</td>\n",
       "      <td>-0.034084</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.038781</td>\n",
       "      <td>-0.093247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096555</td>\n",
       "      <td>-0.260630</td>\n",
       "      <td>0.104516</td>\n",
       "      <td>-0.326727</td>\n",
       "      <td>-0.342016</td>\n",
       "      <td>-0.252645</td>\n",
       "      <td>-0.388910</td>\n",
       "      <td>-0.190061</td>\n",
       "      <td>-0.161088</td>\n",
       "      <td>0.000675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature41</th>\n",
       "      <td>-0.095022</td>\n",
       "      <td>0.454146</td>\n",
       "      <td>-0.733491</td>\n",
       "      <td>-0.439782</td>\n",
       "      <td>-0.480792</td>\n",
       "      <td>0.340459</td>\n",
       "      <td>-0.060208</td>\n",
       "      <td>0.437195</td>\n",
       "      <td>0.426417</td>\n",
       "      <td>-0.061573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445895</td>\n",
       "      <td>-0.496867</td>\n",
       "      <td>0.092979</td>\n",
       "      <td>-0.294027</td>\n",
       "      <td>-0.390985</td>\n",
       "      <td>-0.253542</td>\n",
       "      <td>-0.264755</td>\n",
       "      <td>-0.279056</td>\n",
       "      <td>-0.243887</td>\n",
       "      <td>-0.032594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature42</th>\n",
       "      <td>-0.301285</td>\n",
       "      <td>0.470873</td>\n",
       "      <td>-0.373919</td>\n",
       "      <td>-0.046970</td>\n",
       "      <td>-0.120184</td>\n",
       "      <td>0.311639</td>\n",
       "      <td>-0.207286</td>\n",
       "      <td>0.245033</td>\n",
       "      <td>0.383006</td>\n",
       "      <td>-0.357903</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.229923</td>\n",
       "      <td>-0.323372</td>\n",
       "      <td>0.068342</td>\n",
       "      <td>-0.151824</td>\n",
       "      <td>-0.205075</td>\n",
       "      <td>-0.100491</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>0.176613</td>\n",
       "      <td>-0.027827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature43</th>\n",
       "      <td>-0.403912</td>\n",
       "      <td>-0.028080</td>\n",
       "      <td>0.132449</td>\n",
       "      <td>0.718092</td>\n",
       "      <td>0.364581</td>\n",
       "      <td>-0.015387</td>\n",
       "      <td>-0.323663</td>\n",
       "      <td>-0.611339</td>\n",
       "      <td>-0.123735</td>\n",
       "      <td>-0.327708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.263384</td>\n",
       "      <td>0.305372</td>\n",
       "      <td>0.449280</td>\n",
       "      <td>0.144095</td>\n",
       "      <td>-0.298992</td>\n",
       "      <td>0.559581</td>\n",
       "      <td>0.167938</td>\n",
       "      <td>0.013233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature44</th>\n",
       "      <td>0.457900</td>\n",
       "      <td>-0.268174</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>0.126124</td>\n",
       "      <td>-0.158210</td>\n",
       "      <td>-0.424147</td>\n",
       "      <td>0.105679</td>\n",
       "      <td>-0.084469</td>\n",
       "      <td>0.214562</td>\n",
       "      <td>-0.076684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323372</td>\n",
       "      <td>-0.263384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.111121</td>\n",
       "      <td>-0.038350</td>\n",
       "      <td>-0.446548</td>\n",
       "      <td>0.065558</td>\n",
       "      <td>0.254839</td>\n",
       "      <td>0.417565</td>\n",
       "      <td>0.008623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature45</th>\n",
       "      <td>0.190477</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.292331</td>\n",
       "      <td>0.500078</td>\n",
       "      <td>0.795651</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>0.449574</td>\n",
       "      <td>-0.150242</td>\n",
       "      <td>-0.157244</td>\n",
       "      <td>0.248379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068342</td>\n",
       "      <td>0.305372</td>\n",
       "      <td>-0.111121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.486420</td>\n",
       "      <td>0.438397</td>\n",
       "      <td>0.214540</td>\n",
       "      <td>0.232833</td>\n",
       "      <td>0.188288</td>\n",
       "      <td>0.034562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature46</th>\n",
       "      <td>-0.104100</td>\n",
       "      <td>-0.344950</td>\n",
       "      <td>0.149504</td>\n",
       "      <td>0.282742</td>\n",
       "      <td>0.363971</td>\n",
       "      <td>-0.210808</td>\n",
       "      <td>0.105143</td>\n",
       "      <td>-0.609417</td>\n",
       "      <td>0.382881</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151824</td>\n",
       "      <td>0.449280</td>\n",
       "      <td>-0.038350</td>\n",
       "      <td>0.486420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.048072</td>\n",
       "      <td>-0.007800</td>\n",
       "      <td>0.484351</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>0.029532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature47</th>\n",
       "      <td>0.060931</td>\n",
       "      <td>0.125439</td>\n",
       "      <td>0.348877</td>\n",
       "      <td>0.046970</td>\n",
       "      <td>0.672749</td>\n",
       "      <td>0.274445</td>\n",
       "      <td>0.440815</td>\n",
       "      <td>0.095215</td>\n",
       "      <td>-0.650355</td>\n",
       "      <td>0.703493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205075</td>\n",
       "      <td>0.144095</td>\n",
       "      <td>-0.446548</td>\n",
       "      <td>0.438397</td>\n",
       "      <td>-0.048072</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.432933</td>\n",
       "      <td>-0.267047</td>\n",
       "      <td>-0.290193</td>\n",
       "      <td>0.021299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature48</th>\n",
       "      <td>0.399299</td>\n",
       "      <td>-0.223892</td>\n",
       "      <td>0.503185</td>\n",
       "      <td>-0.042374</td>\n",
       "      <td>0.371995</td>\n",
       "      <td>-0.268461</td>\n",
       "      <td>0.275172</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>-0.381149</td>\n",
       "      <td>0.571077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100491</td>\n",
       "      <td>-0.298992</td>\n",
       "      <td>0.065558</td>\n",
       "      <td>0.214540</td>\n",
       "      <td>-0.007800</td>\n",
       "      <td>0.432933</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.147009</td>\n",
       "      <td>-0.015845</td>\n",
       "      <td>0.006544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature49</th>\n",
       "      <td>-0.188582</td>\n",
       "      <td>-0.243075</td>\n",
       "      <td>-0.056034</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>0.220235</td>\n",
       "      <td>-0.141161</td>\n",
       "      <td>-0.205968</td>\n",
       "      <td>-0.724489</td>\n",
       "      <td>0.336423</td>\n",
       "      <td>-0.375442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>0.559581</td>\n",
       "      <td>0.254839</td>\n",
       "      <td>0.232833</td>\n",
       "      <td>0.484351</td>\n",
       "      <td>-0.267047</td>\n",
       "      <td>-0.147009</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.534921</td>\n",
       "      <td>0.005961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature50</th>\n",
       "      <td>0.202357</td>\n",
       "      <td>-0.268161</td>\n",
       "      <td>0.129323</td>\n",
       "      <td>0.635071</td>\n",
       "      <td>0.205222</td>\n",
       "      <td>-0.157253</td>\n",
       "      <td>-0.160077</td>\n",
       "      <td>-0.390877</td>\n",
       "      <td>0.029528</td>\n",
       "      <td>-0.494857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176613</td>\n",
       "      <td>0.167938</td>\n",
       "      <td>0.417565</td>\n",
       "      <td>0.188288</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>-0.290193</td>\n",
       "      <td>-0.015845</td>\n",
       "      <td>0.534921</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.010096</td>\n",
       "      <td>-0.012585</td>\n",
       "      <td>0.035380</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.036046</td>\n",
       "      <td>-0.017276</td>\n",
       "      <td>0.032228</td>\n",
       "      <td>-0.006604</td>\n",
       "      <td>-0.015055</td>\n",
       "      <td>0.023206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027827</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>0.034562</td>\n",
       "      <td>0.029532</td>\n",
       "      <td>0.021299</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>-0.005076</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "feature1   1.000000 -0.547752  0.188399  0.012195  0.271940 -0.115925   \n",
       "feature2  -0.547752  1.000000 -0.385038 -0.027149 -0.250862  0.293933   \n",
       "feature3   0.188399 -0.385038  1.000000  0.141428  0.547272 -0.574971   \n",
       "feature4   0.012195 -0.027149  0.141428  1.000000  0.472861 -0.132787   \n",
       "feature5   0.271940 -0.250862  0.547272  0.472861  1.000000  0.000291   \n",
       "feature6  -0.115925  0.293933 -0.574971 -0.132787  0.000291  1.000000   \n",
       "feature7   0.464401 -0.139284  0.232475 -0.081232  0.433613 -0.096930   \n",
       "feature8   0.038626  0.491165 -0.057986 -0.596228 -0.296846  0.177762   \n",
       "feature9  -0.069747 -0.031156 -0.607304 -0.090177 -0.386862 -0.008506   \n",
       "feature10  0.401728 -0.229350  0.312914 -0.388336  0.454504  0.049290   \n",
       "feature11  0.096138  0.019389 -0.135064 -0.530351 -0.643403  0.078531   \n",
       "feature12  0.609829 -0.515656  0.408279  0.051377  0.306373 -0.463833   \n",
       "feature13  0.338302  0.244579  0.099412 -0.103372  0.007088 -0.259559   \n",
       "feature14  0.120244  0.090354  0.036521 -0.633142 -0.104831 -0.179031   \n",
       "feature15  0.219771 -0.060663  0.367842  0.534545  0.464247 -0.163877   \n",
       "feature16 -0.443525 -0.086969 -0.070531 -0.270930 -0.342212  0.200168   \n",
       "feature17  0.345141 -0.222458  0.220499  0.666539  0.418473 -0.171863   \n",
       "feature18  0.597160 -0.088139  0.296481  0.392465  0.557378 -0.177745   \n",
       "feature19 -0.435983  0.151814 -0.169533  0.263045 -0.208089  0.066550   \n",
       "feature20  0.321865  0.298031 -0.003482 -0.283698  0.083797 -0.185825   \n",
       "feature21 -0.463633 -0.044965 -0.047522  0.566418 -0.131836 -0.121386   \n",
       "feature22 -0.376396  0.321991 -0.381628 -0.141018 -0.220408  0.188880   \n",
       "feature23 -0.335533 -0.139439 -0.084246 -0.271761 -0.103928  0.313055   \n",
       "feature24  0.371690  0.067753  0.035046 -0.502425  0.020155  0.076618   \n",
       "feature25  0.370928 -0.051172 -0.099386 -0.548653 -0.202895 -0.107014   \n",
       "feature26 -0.457756  0.570357 -0.395298  0.259975 -0.304164  0.115513   \n",
       "feature27 -0.176165  0.030595  0.547509 -0.331018  0.400177 -0.068812   \n",
       "feature28  0.092414 -0.000931  0.043680 -0.745982 -0.398886 -0.060868   \n",
       "feature29  0.167702  0.086193 -0.384840 -0.194567 -0.659489  0.052627   \n",
       "feature30  0.178554 -0.134153 -0.296396 -0.009317 -0.418431 -0.102350   \n",
       "feature31  0.020029  0.013034  0.044561 -0.035395  0.148133  0.320420   \n",
       "feature32 -0.754212  0.296151 -0.327813  0.088829 -0.306245 -0.048163   \n",
       "feature33  0.455294 -0.208393 -0.199649 -0.214642 -0.265098  0.043849   \n",
       "feature34 -0.553739  0.011140 -0.366426 -0.064459 -0.440402  0.136790   \n",
       "feature35 -0.420097  0.350151 -0.588666 -0.100342 -0.415898  0.464779   \n",
       "feature36  0.321800 -0.272734  0.193165 -0.677828  0.053173  0.053209   \n",
       "feature37 -0.269806  0.194719 -0.494252 -0.640702 -0.428423  0.369407   \n",
       "feature38  0.372019 -0.277500  0.764721  0.044792  0.503866 -0.480331   \n",
       "feature39  0.127682 -0.080586  0.196497 -0.238776 -0.068277 -0.610826   \n",
       "feature40 -0.043111  0.154129 -0.187030 -0.371355 -0.470603  0.051060   \n",
       "feature41 -0.095022  0.454146 -0.733491 -0.439782 -0.480792  0.340459   \n",
       "feature42 -0.301285  0.470873 -0.373919 -0.046970 -0.120184  0.311639   \n",
       "feature43 -0.403912 -0.028080  0.132449  0.718092  0.364581 -0.015387   \n",
       "feature44  0.457900 -0.268174  0.005468  0.126124 -0.158210 -0.424147   \n",
       "feature45  0.190477 -0.000032  0.292331  0.500078  0.795651  0.059797   \n",
       "feature46 -0.104100 -0.344950  0.149504  0.282742  0.363971 -0.210808   \n",
       "feature47  0.060931  0.125439  0.348877  0.046970  0.672749  0.274445   \n",
       "feature48  0.399299 -0.223892  0.503185 -0.042374  0.371995 -0.268461   \n",
       "feature49 -0.188582 -0.243075 -0.056034  0.660119  0.220235 -0.141161   \n",
       "feature50  0.202357 -0.268161  0.129323  0.635071  0.205222 -0.157253   \n",
       "target     0.010096 -0.012585  0.035380  0.009333  0.036046 -0.017276   \n",
       "\n",
       "           feature7  feature8  feature9  feature10    ...     feature42  \\\n",
       "feature1   0.464401  0.038626 -0.069747   0.401728    ...     -0.301285   \n",
       "feature2  -0.139284  0.491165 -0.031156  -0.229350    ...      0.470873   \n",
       "feature3   0.232475 -0.057986 -0.607304   0.312914    ...     -0.373919   \n",
       "feature4  -0.081232 -0.596228 -0.090177  -0.388336    ...     -0.046970   \n",
       "feature5   0.433613 -0.296846 -0.386862   0.454504    ...     -0.120184   \n",
       "feature6  -0.096930  0.177762 -0.008506   0.049290    ...      0.311639   \n",
       "feature7   1.000000  0.106962 -0.086672   0.664223    ...     -0.207286   \n",
       "feature8   0.106962  1.000000 -0.184180   0.212485    ...      0.245033   \n",
       "feature9  -0.086672 -0.184180  1.000000  -0.345872    ...      0.383006   \n",
       "feature10  0.664223  0.212485 -0.345872   1.000000    ...     -0.357903   \n",
       "feature11 -0.258756  0.460327 -0.199376  -0.014488    ...     -0.049672   \n",
       "feature12  0.376367 -0.082645 -0.077638   0.349409    ...     -0.593486   \n",
       "feature13  0.326220  0.355070 -0.178748   0.323559    ...     -0.052961   \n",
       "feature14  0.487488  0.372366  0.101307   0.543448    ...      0.001135   \n",
       "feature15  0.001589 -0.290137 -0.583721   0.121013    ...     -0.559008   \n",
       "feature16 -0.330463 -0.190381  0.012713  -0.072481    ...     -0.178910   \n",
       "feature17 -0.119200 -0.409720 -0.285525   0.013674    ...     -0.248691   \n",
       "feature18  0.373224  0.019878 -0.257231   0.342035    ...     -0.015496   \n",
       "feature19 -0.506505 -0.106938  0.422063  -0.702992    ...      0.534566   \n",
       "feature20  0.389223  0.498644 -0.080854   0.394818    ...      0.176314   \n",
       "feature21 -0.643704 -0.524144  0.154246  -0.804474    ...      0.148991   \n",
       "feature22 -0.050567  0.130455  0.306860  -0.015280    ...     -0.072852   \n",
       "feature23 -0.054837 -0.251907 -0.102044   0.094974    ...      0.029624   \n",
       "feature24  0.391690  0.596109 -0.149037   0.583846    ...      0.149569   \n",
       "feature25  0.186299  0.291253 -0.017990   0.534630    ...     -0.330374   \n",
       "feature26 -0.326635  0.157821  0.112881  -0.506269    ...      0.178952   \n",
       "feature27  0.371792  0.254185 -0.347971   0.521857    ...     -0.137821   \n",
       "feature28  0.196650  0.478430 -0.131505   0.402385    ...     -0.394089   \n",
       "feature29 -0.366428  0.381647  0.191786  -0.315199    ...      0.068138   \n",
       "feature30 -0.225260  0.108075  0.606516  -0.449004    ...      0.308751   \n",
       "feature31  0.034278  0.108400 -0.285493   0.227801    ...      0.242642   \n",
       "feature32 -0.391524 -0.287372  0.554866  -0.574009    ...      0.322856   \n",
       "feature33  0.355053  0.237271  0.153278   0.240058    ...     -0.352497   \n",
       "feature34 -0.351830 -0.312179  0.481213  -0.427410    ...      0.099361   \n",
       "feature35 -0.402600  0.072646  0.065415  -0.230045    ...     -0.008314   \n",
       "feature36  0.360115  0.510750 -0.013320   0.583749    ...     -0.221801   \n",
       "feature37 -0.033279  0.326909  0.451322   0.131779    ...      0.125249   \n",
       "feature38  0.405592  0.044537 -0.553116   0.382169    ...     -0.360839   \n",
       "feature39  0.250375  0.009846  0.256453   0.174346    ...     -0.004222   \n",
       "feature40 -0.034084  0.516667  0.038781  -0.093247    ...     -0.096555   \n",
       "feature41 -0.060208  0.437195  0.426417  -0.061573    ...      0.445895   \n",
       "feature42 -0.207286  0.245033  0.383006  -0.357903    ...      1.000000   \n",
       "feature43 -0.323663 -0.611339 -0.123735  -0.327708    ...     -0.229923   \n",
       "feature44  0.105679 -0.084469  0.214562  -0.076684    ...     -0.323372   \n",
       "feature45  0.449574 -0.150242 -0.157244   0.248379    ...      0.068342   \n",
       "feature46  0.105143 -0.609417  0.382881  -0.003520    ...     -0.151824   \n",
       "feature47  0.440815  0.095215 -0.650355   0.703493    ...     -0.205075   \n",
       "feature48  0.275172  0.021948 -0.381149   0.571077    ...     -0.100491   \n",
       "feature49 -0.205968 -0.724489  0.336423  -0.375442    ...      0.021098   \n",
       "feature50 -0.160077 -0.390877  0.029528  -0.494857    ...      0.176613   \n",
       "target     0.032228 -0.006604 -0.015055   0.023206    ...     -0.027827   \n",
       "\n",
       "           feature43  feature44  feature45  feature46  feature47  feature48  \\\n",
       "feature1   -0.403912   0.457900   0.190477  -0.104100   0.060931   0.399299   \n",
       "feature2   -0.028080  -0.268174  -0.000032  -0.344950   0.125439  -0.223892   \n",
       "feature3    0.132449   0.005468   0.292331   0.149504   0.348877   0.503185   \n",
       "feature4    0.718092   0.126124   0.500078   0.282742   0.046970  -0.042374   \n",
       "feature5    0.364581  -0.158210   0.795651   0.363971   0.672749   0.371995   \n",
       "feature6   -0.015387  -0.424147   0.059797  -0.210808   0.274445  -0.268461   \n",
       "feature7   -0.323663   0.105679   0.449574   0.105143   0.440815   0.275172   \n",
       "feature8   -0.611339  -0.084469  -0.150242  -0.609417   0.095215   0.021948   \n",
       "feature9   -0.123735   0.214562  -0.157244   0.382881  -0.650355  -0.381149   \n",
       "feature10  -0.327708  -0.076684   0.248379  -0.003520   0.703493   0.571077   \n",
       "feature11  -0.493935  -0.078923  -0.706883  -0.672002  -0.154228   0.079113   \n",
       "feature12  -0.013211   0.372603   0.203580   0.302833   0.092590   0.257077   \n",
       "feature13  -0.497880   0.370726   0.137964  -0.171727   0.194800   0.548222   \n",
       "feature14  -0.551525  -0.006458  -0.054832   0.006352   0.199166   0.293512   \n",
       "feature15   0.509848   0.044824   0.265767   0.017364   0.440070   0.249114   \n",
       "feature16   0.175948  -0.327939  -0.448799   0.082604  -0.135502  -0.176042   \n",
       "feature17   0.373536   0.304674   0.283194   0.042967   0.203251   0.413869   \n",
       "feature18  -0.166149   0.410553   0.565765  -0.025856   0.299506   0.638493   \n",
       "feature19   0.275250  -0.177407  -0.006232   0.226769  -0.453859  -0.386351   \n",
       "feature20  -0.562458   0.192993   0.167995  -0.287653   0.180481   0.381888   \n",
       "feature21   0.612009  -0.041487  -0.089474   0.234535  -0.442029  -0.407778   \n",
       "feature22   0.236472  -0.073863  -0.050258   0.087745  -0.078268  -0.504756   \n",
       "feature23   0.063716  -0.648455  -0.296552   0.017636   0.231548  -0.068975   \n",
       "feature24  -0.685877  -0.068459   0.053118  -0.353047   0.228330   0.512966   \n",
       "feature25  -0.448970   0.182005  -0.329146  -0.231018   0.119358   0.273669   \n",
       "feature26   0.379910  -0.161853  -0.142573  -0.240645  -0.112629  -0.588743   \n",
       "feature27   0.013982  -0.456938   0.277968   0.218336   0.535161   0.133158   \n",
       "feature28  -0.425787  -0.017666  -0.513683  -0.357921   0.034160  -0.058061   \n",
       "feature29  -0.301915   0.409904  -0.570801  -0.609706  -0.512510  -0.259856   \n",
       "feature30  -0.294692   0.507735  -0.197405  -0.021048  -0.718971  -0.227203   \n",
       "feature31  -0.247544  -0.225504   0.193893  -0.093883   0.286255   0.537011   \n",
       "feature32   0.354040  -0.146960  -0.135110   0.468982  -0.405273  -0.503091   \n",
       "feature33  -0.429490   0.552942  -0.287626  -0.317201  -0.157589  -0.052200   \n",
       "feature34   0.319832  -0.229803  -0.406258   0.235057  -0.348549  -0.584819   \n",
       "feature35   0.300298  -0.330672  -0.452129  -0.340098  -0.030541  -0.632799   \n",
       "feature36  -0.509241   0.058517   0.054418   0.053026   0.137909   0.129580   \n",
       "feature37  -0.189610  -0.319741  -0.250507   0.070341  -0.142086  -0.371319   \n",
       "feature38  -0.154822   0.319292   0.278128  -0.013280   0.358386   0.456054   \n",
       "feature39  -0.373372   0.458122  -0.088522   0.132599  -0.146037   0.271292   \n",
       "feature40  -0.260630   0.104516  -0.326727  -0.342016  -0.252645  -0.388910   \n",
       "feature41  -0.496867   0.092979  -0.294027  -0.390985  -0.253542  -0.264755   \n",
       "feature42  -0.229923  -0.323372   0.068342  -0.151824  -0.205075  -0.100491   \n",
       "feature43   1.000000  -0.263384   0.305372   0.449280   0.144095  -0.298992   \n",
       "feature44  -0.263384   1.000000  -0.111121  -0.038350  -0.446548   0.065558   \n",
       "feature45   0.305372  -0.111121   1.000000   0.486420   0.438397   0.214540   \n",
       "feature46   0.449280  -0.038350   0.486420   1.000000  -0.048072  -0.007800   \n",
       "feature47   0.144095  -0.446548   0.438397  -0.048072   1.000000   0.432933   \n",
       "feature48  -0.298992   0.065558   0.214540  -0.007800   0.432933   1.000000   \n",
       "feature49   0.559581   0.254839   0.232833   0.484351  -0.267047  -0.147009   \n",
       "feature50   0.167938   0.417565   0.188288   0.041401  -0.290193  -0.015845   \n",
       "target      0.013233   0.008623   0.034562   0.029532   0.021299   0.006544   \n",
       "\n",
       "           feature49  feature50    target  \n",
       "feature1   -0.188582   0.202357  0.010096  \n",
       "feature2   -0.243075  -0.268161 -0.012585  \n",
       "feature3   -0.056034   0.129323  0.035380  \n",
       "feature4    0.660119   0.635071  0.009333  \n",
       "feature5    0.220235   0.205222  0.036046  \n",
       "feature6   -0.141161  -0.157253 -0.017276  \n",
       "feature7   -0.205968  -0.160077  0.032228  \n",
       "feature8   -0.724489  -0.390877 -0.006604  \n",
       "feature9    0.336423   0.029528 -0.015055  \n",
       "feature10  -0.375442  -0.494857  0.023206  \n",
       "feature11  -0.514311  -0.185313 -0.036691  \n",
       "feature12  -0.194640   0.010418  0.027212  \n",
       "feature13  -0.368883   0.028862  0.000763  \n",
       "feature14  -0.606318  -0.560935  0.008779  \n",
       "feature15   0.037364   0.086560  0.016418  \n",
       "feature16   0.049640  -0.431017 -0.009206  \n",
       "feature17   0.339297   0.518184 -0.002994  \n",
       "feature18   0.065109   0.406822  0.013050  \n",
       "feature19   0.442040   0.354869 -0.013408  \n",
       "feature20  -0.477385  -0.197078  0.004128  \n",
       "feature21   0.602993   0.568186 -0.016647  \n",
       "feature22   0.192214  -0.525926  0.010204  \n",
       "feature23  -0.084954  -0.259587 -0.016470  \n",
       "feature24  -0.450583  -0.365182 -0.001083  \n",
       "feature25  -0.594811  -0.551928 -0.005370  \n",
       "feature26   0.177397   0.003958 -0.012595  \n",
       "feature27  -0.421144  -0.523829  0.033405  \n",
       "feature28  -0.600755  -0.743030  0.005898  \n",
       "feature29  -0.167601   0.069712 -0.031137  \n",
       "feature30   0.325071   0.417717 -0.016066  \n",
       "feature31  -0.119001   0.138403 -0.017199  \n",
       "feature32   0.375886   0.015832 -0.006344  \n",
       "feature33   0.011337  -0.023694  0.000918  \n",
       "feature34   0.338731  -0.110890 -0.012833  \n",
       "feature35  -0.051414  -0.364611 -0.023753  \n",
       "feature36  -0.410949  -0.404024  0.021873  \n",
       "feature37  -0.267897  -0.766108 -0.007159  \n",
       "feature38  -0.151016   0.269645  0.034330  \n",
       "feature39  -0.078685   0.029344  0.008912  \n",
       "feature40  -0.190061  -0.161088  0.000675  \n",
       "feature41  -0.279056  -0.243887 -0.032594  \n",
       "feature42   0.021098   0.176613 -0.027827  \n",
       "feature43   0.559581   0.167938  0.013233  \n",
       "feature44   0.254839   0.417565  0.008623  \n",
       "feature45   0.232833   0.188288  0.034562  \n",
       "feature46   0.484351   0.041401  0.029532  \n",
       "feature47  -0.267047  -0.290193  0.021299  \n",
       "feature48  -0.147009  -0.015845  0.006544  \n",
       "feature49   1.000000   0.534921  0.005961  \n",
       "feature50   0.534921   1.000000 -0.005076  \n",
       "target      0.005961  -0.005076  1.000000  \n",
       "\n",
       "[51 rows x 51 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(training_data.head(200)[[\"feature1\",\"feature2\",\"feature3\",\"feature4\",\"feature5\",\"feature6\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.694324794 accuracy: 0.498078\n",
      "Epoch: 0002 cost= 0.694322739 accuracy: 0.498078\n",
      "Epoch: 0003 cost= 0.694322739 accuracy: 0.498078\n",
      "Epoch: 0004 cost= 0.694322739 accuracy: 0.498078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f0a4ac1649fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             _, c = sess.run([optimizer_op, cost_op], feed_dict={x: X,\n\u001b[0;32m---> 45\u001b[0;31m                                                                   y: Y})\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#         _, c = sess.run([optimizer_op, cost_op], feed_dict={x: _train_feas,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \"\"\"\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 200\n",
    "batch_size = 200\n",
    "display_step = 1\n",
    "batch = tf.Variable(0, trainable=False)\n",
    "learning_rate_op = tf.train.exponential_decay(\n",
    "    0.02 * 0.01,  # Base learning rate.\n",
    "    batch * batch_size,  # Current index into the dataset.\n",
    "    _train_feas.shape[0],  # Decay step.\n",
    "    0.96,  # Decay rate.\n",
    "    staircase=True)\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 50])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.zeros([50, 1]))\n",
    "pred_op = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "if onehot_option is 1:\n",
    "    y = tf.placeholder(tf.float32, [None, 2])\n",
    "    b = tf.Variable(tf.zeros([2]))\n",
    "    W = tf.Variable(tf.zeros([50, 2]))\n",
    "    pred_op = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "# cost_op=tf.reduce_mean(tf.reduce_sum(- y * tf.log(pred_op) - (1 - y) * tf.log(1 - pred_op), #                                      reduction_indices=[1])+ tf.nn.l2_loss(W)+tf.nn.l2_loss(b))\n",
    "cost_op = tf.reduce_mean(\n",
    "    tf.reduce_sum(-y * tf.log(pred_op), reduction_indices=[1]) + tf.nn.l2_loss(\n",
    "        W) + tf.nn.l2_loss(b))\n",
    "#cost_op = tf.reduce_mean(tf.reduce_sum(-y*tf.log(pred_op), 1))+ 0.01*tf.nn.l2_loss(W) + 0.01*tf.nn.l2_loss(b) \n",
    "#cost_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y,pred_op))+ 0.01*tf.nn.l2_loss(W) + 0.01*tf.nn.l2_loss(b)\n",
    "optimizer_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "    cost_op)\n",
    "# optimizer_op = tf.train.AdamOptimizer(learning_rate).minimize(cost_op)#,global_step=batch)\n",
    "init_op = tf.global_variables_initializer()\n",
    "# Launch the graph\n",
    "result = []\n",
    "with tf.Session() as sess:\n",
    "    sds = SimpleDataSet(_train_feas, _train_onehot_label, batch_size)\n",
    "    total_batch = sds.total_batch()\n",
    "    sess.run(init_op)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        sds.new_epoch()\n",
    "        avg_cost = 0.\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            X, Y = sds.next_batch()\n",
    "            _, c = sess.run([optimizer_op, cost_op], feed_dict={x: X, y: Y})\n",
    "            avg_cost += c / total_batch\n",
    "#         _, c = sess.run([optimizer_op, cost_op], feed_dict={x: _train_feas,\n",
    "#                                                                   y: _train_onehot_label})\n",
    "        correct_prediction_op = tf.equal(\n",
    "            tf.argmax(pred_op, 1), tf.argmax(y, 1))\n",
    "        accuracy_op = tf.reduce_mean(\n",
    "            tf.cast(correct_prediction_op, tf.float32))\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            print \"Epoch:\", \\'%04d\\' % (epoch + 1), \"cost=\", \"{:.9f}\".format(\n",
    "                avg_cost), \"accuracy:\", accuracy_op.eval({\n",
    "                    x: _test_feas,\n",
    "                    y: _test_onehot_label\n",
    "                })\n",
    "        #start to gen tour data\n",
    "    result = pred_op.eval({x: _tour_feas})\n",
    "    print \"Finished!\"\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(data={'t_id':_tid,'probability':result[:,1]})\n",
    "results_df.to_csv(\"../../nb/numerai1/predictions.csv\", index=False,columns=['t_id','probability'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tfrecords_fn=\"../../nb/numerai1/numerai/numerai_training_data.csv.tfrecords\"\n",
    "writer = tf.python_io.TFRecordWriter(train_tfrecords_fn)\n",
    "r=20\n",
    "for i in training_data.itertuples():\n",
    "    feas=[]\n",
    "    for idx in range(1,51):\n",
    "        feas.append((i[idx]))\n",
    "    label=i[51]\n",
    "    print feas,label\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'label': tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n",
    "        'feas': tf.train.Feature(float_list=tf.train.FloatList(value=feas))\n",
    "    }))\n",
    "    writer.write(example.SerializeToString())   \n",
    "    r-=1\n",
    "    if r<=0:\n",
    "        break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#try to read\n",
    "r=2\n",
    "for serialized_example in tf.python_io.tf_record_iterator(train_tfrecords_fn):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_example)\n",
    "    print example.features.feature['feas'].float_list.value\n",
    "    print example.features.feature['label'].float_list.value\n",
    "    r-=1\n",
    "    if r<=0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tfrecords_fn=\"./numerai/numerai_training_data.csv.tfrecords\"\n",
    "def read_and_decode(filename):\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue) \n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label': tf.FixedLenFeature([], tf.float32),\n",
    "                                           'feas' : tf.FixedLenFeature([], tf.float32),\n",
    "                                       })\n",
    "\n",
    "    feas=features['feas']\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    return feas, label\n",
    "feas,label=read_and_decode(train_tfrecords_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 200\n",
    "display_step = 1\n",
    "\n",
    "feas_batch, label_batch = tf.train.batch([feas, label],\n",
    "                                                batch_size=batch_size, capacity=2000)\n",
    "                                                #min_after_dequeue=1000)\n",
    "total_batch=int(len(training_data.index)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    f,l=sess.run([feas_batch,label_batch])\n",
    "    print f,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 50]) #50feas\n",
    "y = tf.placeholder(tf.float32, [None, 1]) #0-1\n",
    "\n",
    "W = tf.Variable(tf.zeros([50, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "pred = tf.nn.sigmoid(tf.matmul(x, W) + b) # sigmoid\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# cost = tf.nn.l2_loss(pred-y,name=\"squared_error_cost\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        print \"epoch:\",epoch\n",
    "        avg_cost = 0.\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            print \"loop:\",i\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: feas_batch.eval(),\n",
    "                                                          y: label_batch.eval()})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "            print \"avg_cost:\",avg_cost\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "# Load the data from the CSV files\n",
    "training_data = pd.read_csv('./numerai/numerai_training_data.csv', header=0)\n",
    "prediction_data = pd.read_csv('./numerai/numerai_tournament_data.csv', header=0)\n",
    "\n",
    "# Transform the loaded CSV data into numpy arrays\n",
    "Y = training_data['target']\n",
    "X = training_data.drop('target', axis=1)\n",
    "t_id = prediction_data['t_id']\n",
    "x_prediction = prediction_data.drop('t_id', axis=1)\n",
    "\n",
    "# This is your model that will learn to predict\n",
    "model = linear_model.LogisticRegression(n_jobs=-1)\n",
    "\n",
    "print(\"Training...\")\n",
    "# Your model is trained on the numerai_training_data\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(\"Predicting...\")\n",
    "# Your trained model is now used to make predictions on the numerai_tournament_data\n",
    "# The model returns two columns: [probability of 0, probability of 1]\n",
    "# We are just interested in the probability that the target is 1.\n",
    "y_prediction = model.predict_proba(x_prediction)\n",
    "results = y_prediction[:, 1]\n",
    "results_df = pd.DataFrame(data={'probability':results})\n",
    "joined = pd.DataFrame(t_id).join(results_df)\n",
    "\n",
    "print(\"Writing predictions to predictions.csv\")\n",
    "# Save the predictions out to a CSV file\n",
    "joined.to_csv(\"predictions.csv\", index=False)\n",
    "# Now you can upload these predictions on numer.ai"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
